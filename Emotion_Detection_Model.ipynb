{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363ab9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9b5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 emotions considered\n",
    "num_of_classes = 5\n",
    "image_rows = 48\n",
    "image_cols = 48\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6f1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generate = ImageDataGenerator(\n",
    "    rotation_range = 30, # in any direction, rotation of image is atmost 30 degrees\n",
    "    rescale = 1./255, #normalizing pixels of the image - to help neurons avoid bias \n",
    "    shear_range = 0.3, \n",
    "    zoom_range = 0.3, #the extent to which zoom is applied\n",
    "    width_shift_range = 0.4, \n",
    "    height_shift_range = 0.4, \n",
    "    horizontal_flip = True, #emotion does not change for the mirror image  \n",
    "    fill_mode = \"nearest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a261ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_generate = ImageDataGenerator(\n",
    "    rescale = 1./255, #normalizing pixels of the image - to help neurons avoid bias \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5504e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_directory = \"dataset/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73611578",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_directory = \"dataset/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f193e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24256 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data_generate.flow_from_directory(\n",
    "    train_data_directory, \n",
    "    color_mode = \"grayscale\",\n",
    "    target_size = (image_rows, image_cols), \n",
    "    batch_size = batch_size,\n",
    "    class_mode = \"categorical\",\n",
    "    shuffle = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99b85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3006 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_data = validation_data_generate.flow_from_directory(\n",
    "    validation_data_directory, \n",
    "    color_mode = \"grayscale\",\n",
    "    target_size = (image_rows, image_cols), \n",
    "    batch_size = batch_size,\n",
    "    class_mode = \"categorical\",\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e54056c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block-1\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\n",
    "                 input_shape=(image_rows,image_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\n",
    "                 input_shape=(image_rows,image_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Block-2\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Block-3\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "#Block-4\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "#Block-5\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "#Block-6\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "#Block-7\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aca4685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 48, 48, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 48, 48, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 24, 24, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 24, 24, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 24, 24, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 12, 12, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 12, 12, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 3, 3, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                147520    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1328037 (5.07 MB)\n",
      "Trainable params: 1325861 (5.06 MB)\n",
      "Non-trainable params: 2176 (8.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb8dbbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = Adam(learning_rate = 0.001), \n",
    "    metrics = ['accuracy', 'Precision', 'Recall']\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc2b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_training_samples = 24256\n",
    "num_of_validation_samples = 3006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef5446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0df0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2, #new learning rate = old learning rate*factor - this new learning rate comes into the picture after 3 epochs of no improvement\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "436a608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.8991 - accuracy: 0.2350 - precision: 0.2287 - recall: 0.0644\n",
      "Epoch 1: val_loss improved from inf to 1.55538, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 1115s 1s/step - loss: 1.8991 - accuracy: 0.2350 - precision: 0.2287 - recall: 0.0644 - val_loss: 1.5554 - val_accuracy: 0.2930 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758/758 [==============================] - ETA: 0s - loss: 1.5795 - accuracy: 0.2822 - precision: 0.2955 - recall: 0.0027\n",
      "Epoch 2: val_loss improved from 1.55538 to 1.54696, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 230s 303ms/step - loss: 1.5795 - accuracy: 0.2822 - precision: 0.2955 - recall: 0.0027 - val_loss: 1.5470 - val_accuracy: 0.3001 - val_precision: 0.5000 - val_recall: 0.0010 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.5486 - accuracy: 0.3016 - precision: 0.4430 - recall: 0.0014\n",
      "Epoch 3: val_loss improved from 1.54696 to 1.51818, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 231s 304ms/step - loss: 1.5486 - accuracy: 0.3016 - precision: 0.4430 - recall: 0.0014 - val_loss: 1.5182 - val_accuracy: 0.3142 - val_precision: 0.5000 - val_recall: 6.7204e-04 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.5201 - accuracy: 0.3272 - precision: 0.5220 - recall: 0.0137\n",
      "Epoch 4: val_loss improved from 1.51818 to 1.49808, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 232s 306ms/step - loss: 1.5201 - accuracy: 0.3272 - precision: 0.5220 - recall: 0.0137 - val_loss: 1.4981 - val_accuracy: 0.3807 - val_precision: 0.4819 - val_recall: 0.2238 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.4527 - accuracy: 0.3611 - precision: 0.6310 - recall: 0.0841\n",
      "Epoch 5: val_loss improved from 1.49808 to 1.46677, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 248s 328ms/step - loss: 1.4527 - accuracy: 0.3611 - precision: 0.6310 - recall: 0.0841 - val_loss: 1.4668 - val_accuracy: 0.4543 - val_precision: 0.5960 - val_recall: 0.2870 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.3622 - accuracy: 0.4231 - precision: 0.6941 - recall: 0.1602\n",
      "Epoch 6: val_loss improved from 1.46677 to 1.31094, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 239s 315ms/step - loss: 1.3622 - accuracy: 0.4231 - precision: 0.6941 - recall: 0.1602 - val_loss: 1.3109 - val_accuracy: 0.4926 - val_precision: 0.6409 - val_recall: 0.3041 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.2856 - accuracy: 0.4625 - precision: 0.7087 - recall: 0.2084\n",
      "Epoch 7: val_loss improved from 1.31094 to 1.29471, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 232s 306ms/step - loss: 1.2856 - accuracy: 0.4625 - precision: 0.7087 - recall: 0.2084 - val_loss: 1.2947 - val_accuracy: 0.5349 - val_precision: 0.6969 - val_recall: 0.3330 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.2365 - accuracy: 0.4916 - precision: 0.7124 - recall: 0.2526\n",
      "Epoch 8: val_loss did not improve from 1.29471\n",
      "758/758 [==============================] - 232s 305ms/step - loss: 1.2365 - accuracy: 0.4916 - precision: 0.7124 - recall: 0.2526 - val_loss: 1.3294 - val_accuracy: 0.5282 - val_precision: 0.6193 - val_recall: 0.4073 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.1904 - accuracy: 0.5131 - precision: 0.7266 - recall: 0.2837\n",
      "Epoch 9: val_loss did not improve from 1.29471\n",
      "758/758 [==============================] - 233s 308ms/step - loss: 1.1904 - accuracy: 0.5131 - precision: 0.7266 - recall: 0.2837 - val_loss: 1.3401 - val_accuracy: 0.5622 - val_precision: 0.6636 - val_recall: 0.4116 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.1514 - accuracy: 0.5340 - precision: 0.7293 - recall: 0.3134\n",
      "Epoch 10: val_loss improved from 1.29471 to 1.29225, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 231s 305ms/step - loss: 1.1514 - accuracy: 0.5340 - precision: 0.7293 - recall: 0.3134 - val_loss: 1.2923 - val_accuracy: 0.5346 - val_precision: 0.6212 - val_recall: 0.4167 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.1306 - accuracy: 0.5464 - precision: 0.7250 - recall: 0.3256\n",
      "Epoch 11: val_loss improved from 1.29225 to 1.28145, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 779s 1s/step - loss: 1.1306 - accuracy: 0.5464 - precision: 0.7250 - recall: 0.3256 - val_loss: 1.2815 - val_accuracy: 0.5716 - val_precision: 0.6408 - val_recall: 0.4328 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.1202 - accuracy: 0.5551 - precision: 0.7299 - recall: 0.3400\n",
      "Epoch 12: val_loss did not improve from 1.28145\n",
      "758/758 [==============================] - 234s 308ms/step - loss: 1.1202 - accuracy: 0.5551 - precision: 0.7299 - recall: 0.3400 - val_loss: 1.3359 - val_accuracy: 0.5652 - val_precision: 0.6336 - val_recall: 0.4399 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.0974 - accuracy: 0.5615 - precision: 0.7323 - recall: 0.3556\n",
      "Epoch 13: val_loss improved from 1.28145 to 1.16831, saving model to EmotionDetectionModel.h5\n",
      "758/758 [==============================] - 233s 308ms/step - loss: 1.0974 - accuracy: 0.5615 - precision: 0.7323 - recall: 0.3556 - val_loss: 1.1683 - val_accuracy: 0.5729 - val_precision: 0.6795 - val_recall: 0.4439 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.0821 - accuracy: 0.5737 - precision: 0.7357 - recall: 0.3758\n",
      "Epoch 14: val_loss did not improve from 1.16831\n",
      "758/758 [==============================] - 232s 307ms/step - loss: 1.0821 - accuracy: 0.5737 - precision: 0.7357 - recall: 0.3758 - val_loss: 1.2474 - val_accuracy: 0.5605 - val_precision: 0.6351 - val_recall: 0.4456 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "758/758 [==============================] - ETA: 0s - loss: 1.0688 - accuracy: 0.5739 - precision: 0.7429 - recall: 0.3778\n",
      "Epoch 15: val_loss did not improve from 1.16831\n",
      "758/758 [==============================] - 234s 308ms/step - loss: 1.0688 - accuracy: 0.5739 - precision: 0.7429 - recall: 0.3778 - val_loss: 1.2372 - val_accuracy: 0.5860 - val_precision: 0.6699 - val_recall: 0.4862 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23e97b29810>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    validation_data = validation_data,\n",
    "    validation_steps = num_of_validation_samples//batch_size, \n",
    "    steps_per_epoch = num_of_training_samples//batch_size,\n",
    "    epochs = 15,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "021710bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd929491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbbf45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
